======================
Affinage par LUDWIG
======================

|colab|

.. |colab| image:: ../images/opencolab.png
    :width: 120
    :height: 20
    :target: https://colab.research.google.com/github/MasrourTawfik/DFMEA-LLM-Enhanced/blob/main/Documentation/colabs/affinage_par_ludwig_notebook_final.ipynb\
..

Maintenant, pour √™tre honn√™te, nous n'avons pas r√©ellement besoin de suivre toutes ces √©tapes √† partir de z√©ro pour affiner un mod√®le de nos jours. Nous pouvons utiliser des frameworks tels que LUDWIG.

.. image:: ../images/Ludwig.png
    :width: 100%
    :align: center
    :alt: LUDWIG framework

Cette section explique comment effectuer le fine-tuning d'un mod√®le de langage en utilisant Ludwig, un outil puissant qui simplifie le cycle de vie de l'apprentissage automatique. Le processus implique l'utilisation d'une configuration de mod√®le sp√©cifique et d'un ensemble de donn√©es pour adapter un mod√®le pr√©-entra√Æn√© √† une t√¢che sp√©cialis√©e.

Configuration
-------------

Avant de commencer, assurez-vous que Ludwig est install√© et configur√© dans votre environnement. Vous devrez √©galement configurer le token du Hugging Face Hub pour acc√©der aux mod√®les pr√©-entra√Æn√©s. Ceci est r√©alis√© en d√©finissant la variable d'environnement HUGGING_FACE_HUB_TOKEN avec votre cl√© API Hugging Face :

.. code-block:: bash

    pip install ludwig ludwig[llm] peft

.. code-block:: python

    import os
    os.environ["HUGGING_FACE_HUB_TOKEN"] = os.getenv('HUGGINGFACE_API_KEY')

Ludwig n√©cessite une configuration YAML qui d√©crit les param√®tres du mod√®le et de l'entra√Ænement. Les √©l√©ments cl√©s de cette configuration incluent :

- model_type : Indique le type de mod√®le. Pour les mod√®les de langage, 'llm' est utilis√©.
- base_model : Sp√©cifie le mod√®le pr√©-entra√Æn√© √† utiliser. Plusieurs options peuvent √™tre fournies, comment√©es pour un changement facile.
- quantization : Applique la quantification du mod√®le pour r√©duire la taille du mod√®le, avec 'bits' sp√©cifiant le niveau de quantification.
- adapter : Un adaptateur permet un fine-tuning efficace. L'adaptateur 'lora' est utilis√© dans cet exemple.
- prompt : D√©finit le mod√®le de prompt pour les donn√©es d'entr√©e.
- input_features et output_features : D√©crivent les structures de donn√©es d'entr√©e et de sortie et les √©tapes de pr√©traitement.
- trainer : Configure le processus d'entra√Ænement, y compris le taux d'apprentissage, la taille du lot et les √©poques.

Voici un extrait de la configuration YAML :

.. code-block:: yaml

    model_type: llm
    base_model: mistralai/Mistral-7B-v0.1
    quantization:
        bits: 4
    adapter:
        type: lora
    prompt:
        template: |
            ### Instruction :
            {instruction}
            ### Entr√©e :
            {input}
            ### R√©ponse :
    input_features:
        - name: prompt
          type: text
    output_features:
        - name: output
          type: text
    trainer:
        type: finetune

Dataset
-------

Le dataset qu'on a utilis√© dans cet exemple c'est Alpaca, c'est une ressource unique destin√©e √† l'entra√Ænement et au fine-tuning des mod√®les de langue pour mieux suivre **les instructions**.

Alpaca comprend 52 000 instructions et d√©monstrations g√©n√©r√©es par l'engine text-davinci-003 d'OpenAI. Ce dataset est sp√©cialement con√ßu pour le fine-tuning instructif des mod√®les de langue, afin d'am√©liorer leur capacit√© √† suivre des instructions.

.. image:: ../images/alpaca.png
    :width: 100%
    :align: center
    :alt: DATASET Alpaca

Entra√Ænement
------------

Pour entra√Æner le mod√®le, chargez la configuration et sp√©cifiez l'ensemble de donn√©es. La classe LudwigModel est utilis√©e avec la m√©thode train :

.. code-block:: python    

    from ludwig.api import LudwigModel
    import yaml
    import logging

    config = yaml.safe_load(config_str)
    model = LudwigModel(config=config, logging_level=logging.INFO)
    results = model.train(dataset="ludwig://alpaca")

Sauvegarde du Mod√®le
--------------------

Apr√®s l'entra√Ænement, sauvegardez le mod√®le pour une utilisation ou un d√©ploiement ult√©rieurs :

.. code:: python

    model.save("results")

Vous pouvez ainsi le sauvegarder directement sur la plateforme huggingface ü§ó

.. code:: bash
    
    python -m ludwig.upload hf_hub --repo_id "Ensamien_001/mistralai-7B-v01-fine-tuned-using-ludwig-4bit" --model_path results/api_experiment_run_2


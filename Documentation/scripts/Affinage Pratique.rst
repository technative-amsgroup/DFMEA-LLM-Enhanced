
Affinage Pratique : Guide Pratique üñ•
===============================================================

|colab|

.. |colab| image:: ../images/opencolab.png
    :width: 120
    :height: 20
    :target: https://colab.research.google.com/github/MasrourTawfik/DFMEA-LLM-Enhanced/blob/main/COMPARAISON.ipynb
..

.. toctree::
   :maxdepth: 2
   :caption: Les √©tapes:

   Pretraitement
   tokenisation
   entrainement
   Ludwig
   evaluation
   
..

Overview
^^^^^^^^^^

Introduction
------------
Pour des fins √©ducatives, nous allons nous lancer dans des activit√©s pratiques afin de consolider fermement les concepts dans nos esprits, en incluant des exemples de code et des tutoriels.

Biblioth√®ques Utilis√©es
-----------------------
- **PyTorch** : Fournit l'interface de plus bas niveau pour l'entra√Ænement de mod√®les de deep learning.
- **Transformers de Hugging Face** : Propose des mod√®les pr√©-entra√Æn√©s et des outils pour le fine-tuning et l'impl√©mentation de mod√®les de traitement du langage naturel.
- **Llama Library**

.. note::
   Ce ne sont pas n√©cessairement les biblioth√®ques que nous avons utilis√©es dans le projet, mais elles pourraient √™tre un bon point de d√©part pour quiconque     
   souhaite explorer.



Dans cet premier exemple, nous allons utiliser la biblioth√®que Llama pour importer quelques mod√®les pr√©-entra√Æn√©s et open source h√©berg√©s sur Hugging Face (dans cet exemple, il s'agit de Llama 2.7). Nous comparerons leurs sorties avant et apr√®s les avoir affin√©s pour la discussion.

Difference entre un mod√®le finetun√© et une architecture pr√©-entrain√©e:
------------------------------------

**architecture pr√©-entrain√©:**

.. code-block:: bash

   pip install llama-library

.. code-block:: python

   from llama import BasicModelRunner

   non_finetuned = BasicModelRunner("meta-llama/Llama-2-7b-hf")
   print(non_finetuned.predict("Tell me how to fix a broken car window"))


**After chat fine-tuning:**

.. code-block:: python

   finetuned_model = BasicModelRunner("meta-llama/Llama-2-7b-chat-hf")
   print(finetuned_model.predict("Tell me how to fix a broken car window"))

.. note::
Nous pouvons affiner un mod√®le plus d'une fois pour des fonctionnalit√©s suppl√©mentaires. Comme on le voit ici, nous avons affin√© le LLM pour la discussion, mais nous pouvons √©galement l'affiner pour jouer, par exemple, le r√¥le d'un assistant dans une entreprise donn√©e, en utilisant leurs informations de questions-r√©ponses ou des conversations pr√©c√©dentes entre assistant et client.


    


